# Backend Configuration for AI Gateway
#
# This file defines the Backend and AIServiceBackend Custom Resources that connect
# the AI Gateway to the llm-katan inference service (Qwen3-0.6B base model).
#
# NOTE: The actual llm-katan Deployment and Service are managed via Kustomize.
# See: deploy/kubernetes/llm-katan/overlays/qwen-default-ns/
#
# Architecture:
#   AIGatewayRoute (gwapi-resources.yaml)
#       ↓ routes based on x-ai-eg-model header
#   AIServiceBackend (this file)
#       ↓ references
#   Backend (this file)
#       ↓ points to
#   llm-katan-qwen Service (created by Kustomize overlay)
#       ↓ targets
#   llm-katan-qwen Pod (Qwen3-0.6B base model)
#
# IMPORTANT: llm-katan does not support LoRA adapters. Domain-specific headers
# (e.g., x-ai-eg-model: math-expert) are received but ignored. All requests are
# processed by the base Qwen3-0.6B model without specialization.
#
# The E2E tests validate routing and connectivity, not specialized model behavior.

---
apiVersion: aigateway.envoyproxy.io/v1alpha1
kind: AIServiceBackend
metadata:
  name: llm-katan-qwen
  namespace: default
spec:
  schema:
    name: OpenAI
  backendRef:
    name: llm-katan-qwen
    kind: Backend
    group: gateway.envoyproxy.io
---
apiVersion: gateway.envoyproxy.io/v1alpha1
kind: Backend
metadata:
  name: llm-katan-qwen
  namespace: default
spec:
  endpoints:
  - fqdn:
      hostname: llm-katan-qwen.default.svc.cluster.local
      port: 8000
